{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KDc_dS2OmTUL"
      },
      "outputs": [],
      "source": [
        "#Importar las librerias matemáticas y de graficos que nos permiten trabajar con datos\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #cargar datos en la meoria en una variable, para poder hacer esto primero debemos caragr el archivo en la memoria de\n",
        " # colab\n",
        "Dataset = pd.read_csv('Data.csv')"
      ],
      "metadata": {
        "id": "JGKxVqFcmhR8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Revisando la base de datos\n",
        "print(Dataset)"
      ],
      "metadata": {
        "id": "q8hTHqxgpJ_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se esta asignando las variables independientes con las cuales trabajaremos para predecir lo que ocurra con\n",
        "#la otra variable, en este caso se consideran todas las filas y las 3 primeras columnas \n",
        "# X=Dataset.iloc[:,:-1].values, los \":\" nos dice que utilizaremos todas las filas y \n",
        "#despues de la coma que utilizaremos todas las columnas menos la ultima\n",
        "X=Dataset.iloc[:,:-1].values\n",
        "Y=Dataset.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "dL6MTp1opiyK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "tCcDXxxebDEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tratamiento de los NaN, se reemplazan dichos datos por los promedios de todos los datos de la muestra\n",
        "#imputer cambia los valores faltantes por los promedios, en axis =0, reemplaza popr promedio de columans, \n",
        "#si se quisiera cambiar filas se utiliza axis = 1\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(missing_values=np.nan, copy=False, strategy=\"mean\", )\n",
        "imp=imp.fit(X[:,1:3])\n",
        "X[:,1:3]=imp.transform(X[:,1:3])"
      ],
      "metadata": {
        "id": "hyi8U9ycrZ6W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Revisando la base de datos de X con los Valores cambiados\n",
        "print(X)"
      ],
      "metadata": {
        "id": "aTxGaPpxwht7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Categoria de datos, Como transformar los datos no numericos a numericos, \n",
        "#codificar a numeros datos categoricas no tienen orden, es una variable nominal\n",
        "# Se deben generar categorias Dummy, para poder tener este tipo de variables\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "labelencoder_X=LabelEncoder()\n",
        "X[:,0]=labelencoder_X.fit_transform(X[:,0])\n",
        "\n"
      ],
      "metadata": {
        "id": "tN4IJXZIHktw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "iZlNW6Doavrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#En esta celda lo que asigan es que a cada pais se asigna como respuesta a una \n",
        "#variable no numerica, lo que hace separa en tres columnas conceros y unos.\n",
        "from sklearn.compose import ColumnTransformer\n",
        "ct = ColumnTransformer(\n",
        "    [('one_hot_encoder', OneHotEncoder(categories='auto'), [0])],\n",
        "    remainder='passthrough'                         \n",
        ")"
      ],
      "metadata": {
        "id": "vhFsxWOjZyVs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = np.array(ct.fit_transform(X), dtype=np.int)\n",
        "labelencoder_y = LabelEncoder()\n",
        "Y = labelencoder_y.fit_transform(Y)\n"
      ],
      "metadata": {
        "id": "SKqQUc0KHkkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "MfVmG3FkHkBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "id": "CRD7GKDhalSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Como dividir el dataset en entrenamiento y conjunto de testing.Lo que nos ayudara \n",
        "#a realizar el modelo para poder realizar el modelo de prediccion\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train, Y_test = train_test_split(X,Y, test_size=0.2,random_state= 0)\n"
      ],
      "metadata": {
        "id": "Ocb2doeKcEiz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train, Y_train)\n",
        "print(X_test, Y_test)"
      ],
      "metadata": {
        "id": "9j-yxPXSeKiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Escalando de variables, es decir, normalización de los datos\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X=StandardScaler()\n",
        "X_train=sc_X.fit_transform(X_train)\n",
        "X_test=sc_X.transform(X_test)\n"
      ],
      "metadata": {
        "id": "e94GHawGe3W7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7GNtc-nOglcm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}